# LaMini-Flan-T5 Training

This repository contains code and configuration for training the [LaMini-Flan-T5](https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M) models with parameter sizes of **248M** and **783M**.

## Overview

The goal is to fine-tune LaMini-Flan-T5 models for task-specific downstream performance. This repo currently supports:

- Training the **248M** and **783M** variants
- Logging training metrics
- Flexible data input for supervised fine-tuning

## Models

| Model Size | Hugging Face Model ID              |
|------------|-------------------------------------|
| 248M       | `MBZUAI/LaMini-Flan-T5-248M`       |
| 783M       | `MBZUAI/LaMini-Flan-T5-783M`       |

