{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning model with Norwegian Index (\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- https://www.kaggle.com/code/aisuko/fine-tuning-t5-small-with-lora \n",
    "- https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/flan-t5-samsum-summarization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer,\n",
    "                          Seq2SeqTrainingArguments, DataCollatorForSeq2Seq)\n",
    "import random\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "\n",
    "model_name = \"MBZUAI/LaMini-Flan-T5-248M\"\n",
    "#model_name = \"MBZUAI/LaMini-Flan-T5-783M\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning paraphrase + intra\n",
    "\n",
    "- Load training and evaluation data\n",
    "- Load t5 model with pretrained weights\n",
    "- Format data to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Inter-Chapter & Instra-Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_intra\n",
      "Dataset({\n",
      "    features: ['QandA'],\n",
      "    num_rows: 4785\n",
      "})\n",
      "\n",
      "train_paraphrase\n",
      "Dataset({\n",
      "    features: ['QandA'],\n",
      "    num_rows: 5680\n",
      "})\n",
      "\n",
      "train_dataset\n",
      "Dataset({\n",
      "    features: ['QandA'],\n",
      "    num_rows: 10465\n",
      "})\n",
      "\n",
      "val_dataset\n",
      "Dataset({\n",
      "    features: ['QandA'],\n",
      "    num_rows: 1136\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_intra = load_dataset(\"json\", data_files=\"Data/TrainData/intra_chapter_dataset.jsonl\",split=\"train\")\n",
    "train_paraphrase = load_dataset(\"json\", data_files=\"Data/TrainData/paraphrase_dataset.jsonl\",split=\"train\")\n",
    "\n",
    "print(f\"train_intra\\n{train_intra}\\n\")\n",
    "print(f\"train_paraphrase\\n{train_paraphrase}\\n\")\n",
    "\n",
    "# Validation\n",
    "val_intra = load_dataset(\"json\", data_files=\"Data/ValData/intra_chapter_dataset.jsonl\",split=\"train\")\n",
    "val_paraphrase = load_dataset(\"json\", data_files=\"Data/ValData/paraphrase_dataset.jsonl\",split=\"train\")\n",
    "\n",
    "# Combined datasets\n",
    "train_dataset = concatenate_datasets([train_intra, train_paraphrase])\n",
    "val_dataset = concatenate_datasets([val_intra, val_paraphrase])\n",
    "\n",
    "print(f\"train_dataset\\n{train_dataset}\\n\")\n",
    "print(f\"val_dataset\\n{val_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Weighted Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 178151424\n",
      "==================================================\n",
      "Train size: 10465\n",
      "Validation size: 1136\n",
      "Max question length: 111\n",
      "Max answer length: 848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1136/1136 [00:01<00:00, 697.22 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10465/10465 [00:14<00:00, 728.71 examples/s]\n",
      "/home/erik/Desktop/MVP/.venv/lib64/python3.13/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded first example from dataset:\n",
      "Question: Instruction: Based on the situation description, identify the most relevant medical chapter and give detailed, structured medical advices.\n",
      " Input: previously experienced anaphylactic shock due to specific insect, allergy history, risk of severe reaction</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Answer: # Relevant Chapters:\n",
      " - 17 Bite / insect sting\n",
      "\n",
      " ## IMPORTANT INFORMATION TO THE CALLER\n",
      " - Help is on the way. I may need to phone you back, so keep this phone free until the medics arrive.\n",
      " - Watch the person all the time. Tell me immediately if anything changes.\n",
      "\n",
      " ## SAFETY AT THE SCENE\n",
      " - Keep out of danger and always ensure the safety of others.\n",
      " - If possible and without risk, get the person to safety.\n",
      "\n",
      " ## MEDICATION\n",
      " - Any medication being used by the person must accompany them at all times.\n",
      " - If the person has an Epi-penÂ® (adults 0.3 mg epinephrine, child 0,15 epinephrine), it can be used before the medics arrive. See advice 6\n",
      "\n",
      " ## ANAPHYLACTIC SHOCK (severe breathing difficulty, pale and clammy skin)\n",
      " - Elevate the legs.\n",
      " - Does the person have allergy medication, e.g. Epi-penÂ® (adults 0.3 mg epinephrine, child 0,15 epinephrine)? Give one dose immediately - follow the instructions on the packet.\n",
      " - If the first dose has not had effect within 5 to 10 mins., you should give a second dose. \n",
      " - After Epinephrine has been administered, all patients (adults and children) should be transported by ambulance to a doctor or hospital.</s>\n",
      "____________________________________________________________________________________________________\n",
      "Inspect dataset sent to training:\n",
      "Question_tokens: tensor([21035,    10,  6719,    30,     8,  1419,  4210,     6,  2862,     8,\n",
      "          167,  2193,  1035,  5800,    11,   428,  3117,     6, 14039,  1035,\n",
      "         1867,     7,     5, 32103,    86,  2562,    10,  3150,  1906,    46,\n",
      "            9,  6941, 22884,  8700,   788,    12,   806, 16304,     6, 23886,\n",
      "          892,     6,  1020,    13,  5274,  6363,     1,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0], device='cuda:0')\n",
      "Answer_tokens: tensor([ 1713, 31484,    17,  8647,     7,    10, 32103,     3,    18,  1003,\n",
      "        12830,    15,     3,    87, 16304,     3, 11026, 32103, 32103,     3,\n",
      "        30345,     3,  5166, 14536,  9156, 28338,  3001,  1853,     3, 21605,\n",
      "        25896, 32103,     3,    18,  5090,    19,    30,     8,   194,     5,\n",
      "           27,   164,   174,    12,   951,    25,   223,     6,    78,   453,\n",
      "           48,   951,   339,   552,     8, 12781,     7,  3658,     5, 32103,\n",
      "            3,    18,  4195,     8,   568,    66,     8,    97,     5,  8779,\n",
      "          140,  2017,     3,    99,   959,  1112,     5, 32103, 32103,     3,\n",
      "        30345,  4646,  6392, 12016,  8043,  1853,  6508, 25576, 32103,     3,\n",
      "           18,  3521,    91,    13,  5129,    11,   373,   766,     8,  1455,\n",
      "           13,   717,     5, 32103,     3,    18,   156,   487,    11,   406,\n",
      "         1020,     6,   129,     8,   568,    12,  1455,     5, 32103, 32103,\n",
      "            3, 30345,  7934, 27447,  8015, 32103,     3,    18,  2372,  7757,\n",
      "          271,   261,    57,     8,   568,   398, 12235,   135,    44,    66,\n",
      "          648,     5, 32103,     3,    18,   156,     8,   568,    65,    46,\n",
      "        12741,    18,  3208,  1732,    41, 28491,     7,     3, 19997,  5453,\n",
      "            3,    15,  3180,    15,   102,   107,  9249,     6,   861,  8014,\n",
      "         1808,     3,    15,  3180,    15,   102,   107,  9249,   201,    34,\n",
      "           54,    36,   261,   274,     8, 12781,     7,  3658,     5,  1610,\n",
      "         1867,   431, 32103, 32103,     3, 30345,     3, 15610,  8023,   476,\n",
      "          434,  9262,  4666,   180,  6299, 10459,    41,     7,  3258,    15,\n",
      "        10882,  8565,     6, 12674,    11,     3, 12818,  2258,  1133,    61,\n",
      "        32103,     3,    18,  8021,   208,   342,     8,  6217,     5, 32103,\n",
      "            3,    18,  3520,     8,   568,    43, 23886,  7757,     6,     3,\n",
      "           15,     5,   122,     5, 12741,    18,  3208,  1732,    41, 28491,\n",
      "            7,     3, 19997,  5453,     3,    15,  3180,    15,   102,   107,\n",
      "         9249,     6,   861,  8014,  1808,     3,    15,  3180,    15,   102,\n",
      "          107,  9249,    61,    58,  6434,    80,  6742,  2017,     3,    18,\n",
      "         1130,     8,  3909,    30,     8, 13531,     5, 32103,     3,    18,\n",
      "          156,     8,   166,  6742,    65,    59,   141,  1504,   441,   305,\n",
      "           12,   335,  3519,     7,     5,     6,    25,   225,   428,     3,\n",
      "            9,   511,  6742,     5,     3, 32103,     3,    18,   621, 10395,\n",
      "          630,   102,   107,  9249,    65,   118, 19092,     6,    66,  1221,\n",
      "           41, 28491,     7,    11,   502,    61,   225,    36, 20626,    57,\n",
      "        25102,    12,     3,     9,  2472,    42,  2833,     5,     1],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15696' max='15696' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15696/15696 2:51:43, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>10.958200</td>\n",
       "      <td>4.777703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.448000</td>\n",
       "      <td>3.031926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.043200</td>\n",
       "      <td>2.258586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.229000</td>\n",
       "      <td>1.740647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.567200</td>\n",
       "      <td>1.427519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.158900</td>\n",
       "      <td>1.219647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.789200</td>\n",
       "      <td>1.101437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.612000</td>\n",
       "      <td>0.930065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.389600</td>\n",
       "      <td>0.804416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.265500</td>\n",
       "      <td>0.723218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.065400</td>\n",
       "      <td>0.685155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.975400</td>\n",
       "      <td>0.636188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.927300</td>\n",
       "      <td>0.604701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.847600</td>\n",
       "      <td>0.560937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.817900</td>\n",
       "      <td>0.502902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.685600</td>\n",
       "      <td>0.456971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>0.469424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>0.407237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.606800</td>\n",
       "      <td>0.406228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>0.363752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.511000</td>\n",
       "      <td>0.348314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>0.352528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>0.326276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.394600</td>\n",
       "      <td>0.328773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.424800</td>\n",
       "      <td>0.314258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.382600</td>\n",
       "      <td>0.308708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.414300</td>\n",
       "      <td>0.275707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.363500</td>\n",
       "      <td>0.277615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.344100</td>\n",
       "      <td>0.261126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.320800</td>\n",
       "      <td>0.267626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.240217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses saved to ./Data/Plots/weighted-intra-paraphrase-MBZUAI-LaMini-Flan-T5-783M-finetuned (ep=3, bs=1, lr=0.0005, wd=0.001, grad_acc=2).json\n"
     ]
    }
   ],
   "source": [
    "bs = 1\n",
    "lr = 5e-4\n",
    "epochs = 3\n",
    "wd = 0.001\n",
    "log_steps = 500\n",
    "gradient_accumulation_steps = 2\n",
    "max_context_length = 1024\n",
    "last_layer_train = True\n",
    "\n",
    "output_dir = f\"./Models/weighted-intra-paraphrase-{model_name}-finetuned (ep={epochs}, bs={bs}, lr={lr}, wd={wd}, grad_acc={gradient_accumulation_steps}\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.dropout_rate = 0.3\n",
    "config.max_position_embeddings = max_context_length\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "\tmodel_name,\n",
    "\tconfig=config,\n",
    "\tdevice_map=\"auto\",\n",
    ")\n",
    "\n",
    "if last_layer_train:\n",
    "\n",
    "\tenc_unfreeze_start = 18   # unfreeze encoder blocks 18â€“23 (last 6 layers)\n",
    "\tdec_unfreeze_start = 20   # unfreeze decoder blocks 20â€“23 (last 4 layers)\n",
    "\n",
    "\toutput_dir += f\", last_enc_layers={24-enc_unfreeze_start}, last_dec_layers={24-dec_unfreeze_start})\"\n",
    "\n",
    "\tfor name, param in model.named_parameters():\n",
    "\t\tif any(f\"encoder.block.{i}\" in name for i in range(enc_unfreeze_start, 24)):\n",
    "\t\t\tparam.requires_grad = True\n",
    "\t\telif any(f\"decoder.block.{i}\" in name for i in range(dec_unfreeze_start, 24)):\n",
    "\t\t\tparam.requires_grad = True\n",
    "\t\telif \"lm_head\" in name:\n",
    "\t\t\tparam.requires_grad = True\n",
    "\t\telse:\n",
    "\t\t\tparam.requires_grad = False\n",
    "\n",
    "else:\n",
    "\toutput_dir += \")\"\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {num_trainable_params}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"Ã¦\", \"Ã¸\", \"Ã¥\", \"\\n\"]})\n",
    "tokenizer.model_max_length = max_context_length\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")\n",
    "\n",
    "max_train_question_length = max(len(tokenizer(sample[\"QandA\"][0][\"content\"], truncation=True)['input_ids']) for sample in train_dataset)\n",
    "max_train_answer_length = max(len(tokenizer(sample[\"QandA\"][1][\"content\"], truncation=True)['input_ids']) for sample in train_dataset)\n",
    "\n",
    "max_val_question_length = max(len(tokenizer(sample[\"QandA\"][0][\"content\"], truncation=True)['input_ids']) for sample in val_dataset)\n",
    "max_val_answer_length = max(len(tokenizer(sample[\"QandA\"][1][\"content\"], truncation=True)['input_ids']) for sample in val_dataset)\n",
    "\n",
    "max_question_length = max(max_train_question_length, max_val_question_length)\n",
    "max_answer_length = max(max_train_answer_length, max_val_answer_length)\n",
    "\n",
    "print(f\"Max question length: {max_question_length}\")\n",
    "print(f\"Max answer length: {max_answer_length}\")\n",
    "\n",
    "\n",
    "def formatting_prompts_func(sample):\n",
    "\tQA = sample[\"QandA\"]  # List with user and assistant responses\n",
    "\tsymptoms = QA[0]['content'].strip()\n",
    "\tanswer = QA[1]['content'].strip()\n",
    "\n",
    "\t\n",
    "\tquestion = (\n",
    "\t\t\"Instruction: Based on the situation description, identify the most relevant medical chapter \"\n",
    "\t\t\"and give detailed, structured medical advices.\\n\"\n",
    "\t\tf\"Input: {symptoms}\"\n",
    "\t)\n",
    "\n",
    "\n",
    "\t# Tokenize question (inputs)\n",
    "\ttokens = tokenizer(question, max_length=max_question_length, padding=\"max_length\", truncation=True)\n",
    "\t# Tokenize answer (targets)\n",
    "\tanswer_enc = tokenizer(text_target=answer, max_length=max_answer_length, padding=\"max_length\", truncation=True)\n",
    "\t# Replace pad tokens with -100 for the labels so that loss is not computed on them\n",
    "\tlabels = [-100 if token_id == tokenizer.pad_token_id else token_id for token_id in answer_enc[\"input_ids\"]]\n",
    "\ttokens[\"labels\"] = labels\n",
    "\n",
    "\tadvice_sample_weight = QA[1]['advice_sample_weight']\n",
    "\tchapter_sample_weight = QA[1]['chapter_sample_weight']\n",
    "\n",
    "\tsample_weight = 0.6 * advice_sample_weight + 0.4 * chapter_sample_weight\n",
    "\ttokens[\"sample_weight\"] = sample_weight\n",
    "\n",
    "\treturn tokens\n",
    "\n",
    "val_dataset = val_dataset.map(formatting_prompts_func, batched=False).remove_columns([\"QandA\"])\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=False).remove_columns([\"QandA\"])\n",
    "train_dataset = train_dataset.shuffle(seed=seed)\n",
    "\n",
    "\n",
    "class DataCollatorForSeq2SeqWithWeights(DataCollatorForSeq2Seq):\n",
    "\tdef __call__(self, features):\n",
    "\t\tsample_weights = [feature.pop(\"sample_weight\") for feature in features]\n",
    "\t\tbatch = super().__call__(features)\n",
    "\t\tbatch[\"sample_weight\"] = torch.tensor(sample_weights, dtype=torch.float)\n",
    "\t\treturn batch\n",
    "\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2SeqWithWeights(\n",
    "\ttokenizer=tokenizer,\n",
    "\tmodel=model,\n",
    "\tlabel_pad_token_id=-100,\n",
    "\tpad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "class CustomTrainer(Seq2SeqTrainer):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tsuper().__init__(*args, **kwargs)\n",
    "\t\tself.train_losses = [] \n",
    "\t\tself.eval_losses = []\n",
    "\n",
    "\tdef log(self, logs, *args, **kwargs):\n",
    "\t\tif \"loss\" in logs:\n",
    "\t\t\tself.train_losses.append(logs[\"loss\"])\n",
    "\n",
    "\t\tif \"eval_loss\" in logs:\n",
    "\t\t\tself.eval_losses.append(logs[\"eval_loss\"])\n",
    "\n",
    "\t\tsuper().log(logs, *args, **kwargs)\n",
    "\n",
    "\tdef get_train_losses(self):\n",
    "\t\treturn self.train_losses\n",
    "\t\n",
    "\tdef get_eval_losses(self):\n",
    "\t\treturn self.eval_losses\n",
    "\t\n",
    "\n",
    "\tdef compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "\t\tsample_weight = inputs.pop(\"sample_weight\", None)\n",
    "\t\n",
    "\t\tif sample_weight is None:\n",
    "\t\t\traise ValueError(\"sample_weight must be provided.\")\n",
    "\n",
    "\t\toutputs = model(**inputs)\n",
    "\t\tloss = outputs.loss\n",
    "\t\tweighted_loss = (loss * sample_weight.to(loss.device)).mean()\n",
    "\n",
    "\t\treturn (weighted_loss, outputs) if return_outputs else weighted_loss\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "\tper_device_train_batch_size=bs,\n",
    "\tpredict_with_generate=True,\n",
    "\tnum_train_epochs=epochs,\n",
    "\tlogging_steps=log_steps,\n",
    "\n",
    "\tevaluation_strategy=\"steps\",\n",
    "\teval_steps=log_steps,\n",
    "\t\n",
    "\tlearning_rate=lr,\n",
    "\tweight_decay=wd,\n",
    "\tlr_scheduler_type=\"constant\",\n",
    "\toptim=\"paged_adamw_32bit\",\n",
    "\tgradient_accumulation_steps=gradient_accumulation_steps,\n",
    "\tsave_strategy=\"no\",\n",
    "\t\n",
    "\t#save_strategy=\"steps\",\n",
    "\t#load_best_model_at_end=True,\n",
    "\tremove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "\tmodel=model,\n",
    "\ttrain_dataset=train_dataset,\n",
    "\teval_dataset=val_dataset,\n",
    "\targs=training_args,\n",
    "\tdata_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Inspect the first training batch\n",
    "dataloader = trainer.get_train_dataloader()\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "input_ids = batch[\"input_ids\"][0]\n",
    "label_ids = batch[\"labels\"][0][batch[\"labels\"][0] != -100]\n",
    "\n",
    "print(\"\\nDecoded first example from dataset:\")\n",
    "question_text = tokenizer.decode(input_ids, max_length=max_question_length, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "answer_text = tokenizer.decode(label_ids, max_length=max_answer_length, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "print(\"Question:\", question_text)\n",
    "print(\"Answer:\", answer_text)\n",
    "print(\"_\"*100)\n",
    "\n",
    "# Also inspect the batch from dataloader\n",
    "print(\"Inspect dataset sent to training:\")\n",
    "print(\"Question_tokens:\", input_ids)\n",
    "print(\"Answer_tokens:\", label_ids)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)  # This saves the model and config\n",
    "tokenizer.save_pretrained(output_dir)  # This saves the tokenizer files\n",
    "\n",
    "# Plot training loss curve\n",
    "train_losses = trainer.get_train_losses()\n",
    "eval_losses = trainer.get_eval_losses()\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(0, len(train_losses), len(train_losses) // len(eval_losses)), eval_losses, label=\"Eval Loss\")\n",
    "plt.title('Training & Evaluation Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "losses_dict = {\n",
    "\t\"train_losses\": train_losses,\n",
    "\t\"eval_losses\": eval_losses\n",
    "}\n",
    "# Save the dictionary to a JSON file\n",
    "losses_file_path = f'./Data/Plots/weighted-intra-paraphrase-{model_name.replace(\"/\",\"-\")}-finetuned (ep={epochs}, bs={bs}, lr={lr}, wd={wd}, grad_acc={gradient_accumulation_steps}, log_steps={log_steps}).json'\n",
    "with open(losses_file_path, 'w') as f:\n",
    "\tjson.dump(losses_dict, f, indent=2)\n",
    "print(f\"Losses saved to {losses_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_params(file_path):\n",
    "\t# Extract parameters from the peft_model_path\n",
    "\tparams_string = re.search(r\"\\((.*?)\\)\", file_path).group(1)\n",
    "\tparams = dict(re.findall(r\"(\\w+)=([^\\s,]+)\", params_string))\n",
    "\tfor key, value in params.items():\n",
    "\t\tif value.isdigit():\n",
    "\t\t\tparams[key] = int(value)\n",
    "\t\telse:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tparams[key] = float(value)\n",
    "\t\t\texcept ValueError:\n",
    "\t\t\t\tpass\n",
    "\treturn params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# Path to your saved JSON\n",
    "# losses_file_path = './Data/Plots/intra-paraphrase-MBZUAI-LaMini-Flan-T5-248M-finetuned (ep=1, bs=1, lr=0.001, wd=0.005, grad_acc=8).json'\n",
    "# losses_file_path = \"./Data/Plots/weighted-intra-paraphrase-MBZUAI-LaMini-Flan-T5-248M-finetuned (ep=3, bs=1, lr=0.0005, wd=0.001, grad_acc=2).json\"\n",
    "losses_file_path = \"./Data/Plots/weighted-intra-paraphrase-MBZUAI-LaMini-Flan-T5-783M-finetuned (ep=3, bs=1, lr=0.0005, wd=0.001, grad_acc=2, log_steps=500).json\"\n",
    "\n",
    "params = extract_params(losses_file_path)\n",
    "hyperparams = f\"Hyperparameters:\\n\\nmodel={model_name}\\nepochs={params[\"ep\"]}\\nbatch_size={params[\"bs\"]}\\nlearning_rate={params[\"lr\"]}\\nweight_decay={params[\"wd\"]}\\ngradient_accumulation_steps={params[\"grad_acc\"]}\\nlog_steps={params[\"log_steps\"]}\"\n",
    "\n",
    "with open(losses_file_path, 'r') as f:\n",
    "\tlosses_dict = json.load(f)\n",
    "\n",
    "train_losses = losses_dict[\"train_losses\"]\n",
    "eval_losses = losses_dict[\"eval_losses\"]\n",
    "\n",
    "# Compute x values in terms of real training steps\n",
    "x_values = [i * params[\"log_steps\"] for i in range(1, len(train_losses) + 1)]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(x_values, train_losses, label='Training Loss', color='#1f77b4', linewidth=2.5, marker='o', markersize=6)\n",
    "plt.plot(x_values, eval_losses, label='Validation Loss', color='#ff7f0e', linewidth=2.5, marker='s', markersize=6)\n",
    "\n",
    "plt.title('Training vs Validation Loss', fontsize=18, fontweight='bold', pad=20)\n",
    "\n",
    "# Add hyperparameters text box\n",
    "plt.gcf().text(0.4, 0.75, hyperparams, fontsize=13, va='top', ha='left', linespacing=2)\n",
    "\n",
    "plt.xlabel('Training Steps', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "\n",
    "# Use MultipleLocator to set the x-ticks to multiples \n",
    "plt.gca().xaxis.set_major_locator(MultipleLocator(params[\"log_steps\"]*2))\n",
    "\n",
    "plt.xticks(fontsize=12, rotation=45)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(visible=True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.legend(fontsize=12, loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/Desktop/MVP/.venv/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32104, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32104, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32104, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32104, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "#model_path = \"./Models/weighted-intra-paraphrase-MBZUAI/LaMini-Flan-T5-783M-finetuned (ep=3, bs=1, lr=0.0005, wd=0.001, grad_acc=2, last_enc_layers=6, last_dec_layers=4)\"\n",
    "model_path = \"./Models/weighted-intra-paraphrase-MBZUAI/LaMini-Flan-T5-248M-finetuned (ep=3, bs=1, lr=0.0005, wd=0.001, grad_acc=2)\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/erik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/erik/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/erik/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset\n",
      "Dataset({\n",
      "    features: ['QandA'],\n",
      "    num_rows: 1136\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'QandA': [{'role': 'user',\n",
       "   'content': 'Symptoms: Numbness or paralysis in various body parts, fatigue, decreased alertness, sensory issues like tingling, and a decline in consciousness following a diving session, all developing within a day.\\nCause: Potential decompression sickness or barotrauma resulting from scuba diving.',\n",
       "   'chapter': None,\n",
       "   'original_advices': None,\n",
       "   'advice_sample_weight': None,\n",
       "   'chapter_sample_weight': None},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"# Relevant Chapters: \\n- 09 Scuba diving accident\\n\\n## IMPORTANT INFORMATION TO THE CALLER\\n- Help is on the way. I may need to phone you back, so keep this phone free until the medics arrive.\\n- Watch the casualty all the time. Tell me immediately if anything changes.\\n\\nNB!\\n- Take care of all valves and tanks.\\n- You need to send the diver's log, depth gauge and dive computer with the casualty if possible.\\n\\n## SAFETY AT THE SCENE\\n- Keep out of danger and always ensure the safety of others.\\n- If possible and without risk, get the casualties to safety.\\n- Try to get an overall picture and give me more information as soon as you can.\\n\\n## SUSPECT DECOMPRESSION SICKNESS / THE BENDS\\n- The casualty must Lay quietly on the left side with the head elevated.\\n- If alert and not nauseous: Give plenty of fluids to drink.\\n\\n## IF SYMPTOMS AFTER A DIVE\\n- Give 100 % oxygen if available.\\n- Preferably 10L / min. in an anesthetic facemask (or an O2 diving regulator).\\n\\n## DIFFICULT TO WAKE, DROWSY, SLEEPY (altered level of consciousness)\\n- Lay the person on his / her side.\\n- Make sure the airway is free.\\n- Keep warm with blankets (or clothing) over and under the person, and keep the person in a sheltered area.\\n\\n## AVOID ALL LOSS OF HEAT\\n- Remove wet clothes and wrap the casualty in dry blankets or dry clothes - use as many as you can.\\n- Place an insulating layer under him / her if possible.\\n- Try to create shelter from the elements (weather / wind), go indoors if possible.\\n\\nLIFE-SAVING FIRST AID FOR HYPOTHERMIA\\n\\nCold casualty barely responsive / unresponsive:\\n- Make sure the airway is free and take 60 seconds to check if the casualty is breathing.\\n- If s/he is breathing normally, place flat in the recovery position.\\n- Check breathing at regular intervals (every minute).\\n- Avoid any abrupt movements or changes in the casualty's position, this could lead to cardiac arrest.\\n\\nAvoid any more heat loss:\\n- As soon as insulating, windproof textiles are available to wrap the casualty in, cut away all wet clothing. Alternatively, leave the casualty in the wet clothes and pack him/her in windproof / steam proof textiles.\\n- If possible, get the casualty to a sheltered place, preferably indoors in a warm room.\\n\\nFully alert, unharmed casualties:\\n- Give plenty of warm, sugary drink (non- alcoholic).\\n- Do not rub the affected areas.\\n- Give oxygen if obtainable.\\n- Take the casualty's temperature if you have a thermometer, and it is practical to do so.\",\n",
       "   'chapter': '',\n",
       "   'original_advices': '',\n",
       "   'advice_sample_weight': 5.305269885862918,\n",
       "   'chapter_sample_weight': 3.254886262391234}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_intra = load_dataset(\"json\", data_files=\"Data/TestData/intra_chapter_dataset.jsonl\", split=\"train\")\n",
    "test_paraphrase = load_dataset(\"json\", data_files=\"Data/TestData/paraphrase_dataset.jsonl\", split=\"train\")\n",
    "test_dataset = concatenate_datasets([test_intra, test_paraphrase])\n",
    "print(f\"test_dataset\\n{test_dataset}\")\n",
    "\n",
    "display(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vague dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 14758.28 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset\n",
      "Dataset({\n",
      "    features: ['QandA'],\n",
      "    num_rows: 35\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'QandA': [{'content': \"Instruction: Based on the situation description, identify the most relevant medical chapter and give detailed, structured medical advices.\\nInput: Condition: In distressed, pointing at the their upper chest or neck\\nObservations: Their respiration comes in irregular bursts, with noticeable gaps between each inhale; eyes flicker nervously around the surroundings; brief, dry coughs break the silence, usually chatty but now mostly quiet\\nCause: Had a meal not long ago, though it's uncertain if that's related\",\n",
       "   'role': 'user'},\n",
       "  {'content': '4', 'role': 'assistant'}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"json\", data_files=\"Data/TestData/vague_data.jsonl\", split=\"train\")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "\tQandA_list = []\n",
    "\tfor input_text, label in zip(examples[\"input_text\"], examples[\"label\"]):\n",
    "\t\tinput_text = (\n",
    "\t\t\t\"Instruction: Based on the situation description, identify the most relevant medical chapter \"\n",
    "\t\t\t\"and give detailed, structured medical advices.\\n\"\n",
    "\t\t\tf\"Input: {input_text}\"\n",
    "\t\t)\n",
    "\n",
    "\t\tQandA = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input_text,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": str(label),\n",
    "            }\n",
    "        ]\n",
    "\n",
    "\t\tQandA_list.append(QandA)\n",
    "\n",
    "\treturn {\"QandA\": QandA_list}\n",
    "\n",
    "test_dataset = test_dataset.map(formatting_prompts_func, batched=True).remove_columns([\"label\", \"input_text\"])\n",
    "\n",
    "print(f\"test_dataset\\n{test_dataset}\")\n",
    "display(test_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Batched Evaluation (Inference) on Test Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:07<00:00,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference results saved to ./Data/Evaluation/VagueTestweighted-intra-paraphrase-MBZUAI-LaMini-Flan-T5-248M-finetuned (ep=3, bs=1, lr=0.0005, wd=0.001, grad_acc=2).json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm  \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def extract_first_number(text):\n",
    "\tmatch = re.search(r'\\b\\d+\\b', text)\n",
    "\tif match:\n",
    "\t\treturn match.group().strip()\n",
    "\treturn \"\"\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "input_texts = []\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "for entry in test_dataset:\n",
    "\tQandA = entry[\"QandA\"]\n",
    "\tsymptoms = QandA[0][\"content\"]\n",
    "\tlabel = QandA[1][\"content\"]\n",
    "\t\n",
    "\tinput_text = (\n",
    "\t\t\"Instruction: Based on the situation description, identify the most relevant medical chapter \"\n",
    "\t\t\"and give detailed, structured medical advices.\\n\"\n",
    "\t\tf\"Input: {symptoms}\"\n",
    "\t)\n",
    "\n",
    "\tinput_texts.append(input_text)\n",
    "\treferences.append(extract_first_number(label.strip()))\n",
    "\n",
    "# Create batches manually\n",
    "dataloader = DataLoader(list(zip(input_texts, references)), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Run inference\n",
    "for batch in tqdm(dataloader, desc=\"Running Batched Evaluation (Inference) on Test Data\"):\n",
    "\tbatch_input_texts, batch_labels = batch\n",
    "\n",
    "\t# Tokenize entire batch\n",
    "\tinputs = tokenizer(list(batch_input_texts), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\t# Generate outputs\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = model.generate(\n",
    "\t\t\t**inputs,\n",
    "\t\t\tmax_length=50,\n",
    "\t\t\tdo_sample=False,\n",
    "\t\t)\n",
    "\n",
    "\tdefault_special_token_ids = tokenizer.convert_tokens_to_ids([token for token in tokenizer.special_tokens_map.values() if not isinstance(token, list)])\n",
    "\n",
    "\tfor output in outputs:\n",
    "\t\tfiltered_tokens = [token for token in output if token not in default_special_token_ids]\n",
    "\t\tdecoded_output = tokenizer.decode(filtered_tokens, skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
    "\t\tprediction = extract_first_number(decoded_output.strip())\n",
    "\t\tpredictions.append(prediction)\n",
    "\n",
    "# Prepare data for saving\n",
    "output_data = {\n",
    "    #\"input_texts\": input_texts,\n",
    "    \"references\": references,\n",
    "    \"predictions\": predictions\n",
    "}\n",
    "\n",
    "# Save the data to a JSON file\n",
    "output_file = \"./Data/Evaluation/VagueTest\" + model_path.replace(\"./Models/\",\"\").replace(\"/\", \"-\") + \".json\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(f\"Inference results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract chapter numbers from references and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_number(text):\n",
    "\tmatch = re.search(r'\\b\\d+\\b', text)\n",
    "\tif match:\n",
    "\t\treturn match.group().strip()\n",
    "\treturn \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/Evaluation/weighted-intra-paraphrase-MBZUAI-LaMini-Flan-T5-248M-finetuned (ep=3, bs=1, lr=0.0005, wd=0.001, grad_acc=2).json\n",
      "number of samples in the dataset: 1136\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "output_file = \"./Data/Evaluation/\" + model_path.replace(\"./Models/\",\"\").replace(\"/\", \"-\") + \".json\"\n",
    "print(output_file)\n",
    "with open(output_file, \"r\") as f:\n",
    "\tdata = json.load(f)\n",
    "\n",
    "input_texts = data[\"input_texts\"]\n",
    "references = data[\"references\"]\n",
    "predictions = data[\"predictions\"]\n",
    "\n",
    "print(\"number of samples in the dataset:\", len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ ROUGE Results:\n",
      "rouge1: 0.7420\n",
      "rouge2: 0.6973\n",
      "rougeL: 0.6921\n",
      "rougeLsum: 0.7381\n",
      "\n",
      "ðŸ”¹ BLEU Score:\n",
      "BLEU: 0.7088\n",
      "\n",
      "ðŸ”¹ METEOR Score:\n",
      "METEOR: 0.7065\n"
     ]
    }
   ],
   "source": [
    "# Compute ROUGE\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "# ROUGE\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "print(\"ðŸ”¹ ROUGE Results:\")\n",
    "for metric, score in rouge_results.items():\n",
    "    print(f\"{metric}: {score:.4f}\")\n",
    "    \n",
    "# BLEU\n",
    "bleu_results = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "print(\"\\nðŸ”¹ BLEU Score:\")\n",
    "print(f\"BLEU: {bleu_results['bleu']:.4f}\")\n",
    "\n",
    "# METEOR\n",
    "meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "print(\"\\nðŸ”¹ METEOR Score:\")\n",
    "print(f\"METEOR: {meteor_score['meteor']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Accuracy Score:\n",
      "Accuracy: 0.805\n",
      "\n",
      "ðŸ”¹ Precision Score:\n",
      "Precision: 0.856\n",
      "\n",
      "ðŸ”¹ Recall Score:\n",
      "Recall: 0.803\n",
      "\n",
      "ðŸ”¹ F1 Score:\n",
      "F1: 0.805\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")  # Precision metric\n",
    "recall_metric = evaluate.load(\"recall\")  # Recall metric\n",
    "\n",
    "# Tokenize\n",
    "references = [int(extract_first_number(label)) for label in data[\"references\"]]\n",
    "predictions = [int(extract_first_number(pred)) for pred in data[\"predictions\"]]\n",
    "\n",
    "# Compute Accuracy\n",
    "accuracy_score = accuracy_metric.compute(predictions=predictions, references=references)\n",
    "print(\"\\nðŸ”¹ Accuracy Score:\")\n",
    "print(f\"Accuracy: {accuracy_score['accuracy']:.3f}\")\n",
    "\n",
    "# Compute Precision\n",
    "precision_score = precision_metric.compute(predictions=predictions, references=references,average=\"macro\", zero_division=0)\n",
    "print(\"\\nðŸ”¹ Precision Score:\")\n",
    "print(f\"Precision: {precision_score['precision']:.3f}\")\n",
    "\n",
    "# Compute Recall\n",
    "recall_score = recall_metric.compute(predictions=predictions, references=references,average=\"macro\", zero_division=0)\n",
    "print(\"\\nðŸ”¹ Recall Score:\")\n",
    "print(f\"Recall: {recall_score['recall']:.3f}\")\n",
    "\n",
    "# Compute F1\n",
    "f1_score = f1_metric.compute(predictions=predictions, references=references,average=\"macro\")\n",
    "print(\"\\nðŸ”¹ F1 Score:\")\n",
    "print(f\"F1: {f1_score['f1']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<User Query>:\n",
      "\n",
      "falling from ladder, broken leg and bleeding\n",
      "\n",
      "\n",
      "<Assistant>:\n",
      "# Relevant Chapters: \n",
      " - 11 Accident / injury\n",
      "\n",
      " ## IMPORTANT INFORMATION TO THE CALLER\n",
      " - THINK SAFETY FIRST!\n",
      " - Help is on the way. I may need to phone you back, so keep this phone free until the medics arrive.\n",
      " - Comfort and reassure the casualty.\n",
      " - Avoid any loss of body heat, cover the person with blankets and place on an insulating layer if possible. Find shelter and shield from the wind.\n",
      " - Watch the casualty all the time. Tell me immediately if anything changes.\n",
      "\n",
      " ## SAFETY AT THE SCENE\n",
      " - Keep out of danger and always ensure the safety of others.\n",
      " - If possible and without risk, get the person to safety.\n",
      " - Try to get an overall picture and give me more information as soon as you can.\n",
      "\n",
      " ## DIFFICULT TO WAKE, DROWSY, SLEEPY (altered level of consciousness)\n",
      " - Lay the person on his / her side.\n",
      " - Make sure the airway is free.\n",
      " - Keep warm with blankets (or clothing) over and under the person, and keep the person in a sheltered area.\n",
      "\n",
      " ## MAJOR BLEEDING AND OPEN INJURIES / WOUNDS\n",
      " - Stop the bleeding by putting pressure on the wound with a clean cloth until the bleeding stops.\n",
      " - Maintain a free airway and check s/he can breathe freely.\n",
      " - Cover the wound with clean cloths or bandages.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "falling from ladder, broken leg and bleeding\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "default_special_token_ids = tokenizer.convert_tokens_to_ids([token for token in tokenizer.special_tokens_map.values() if not isinstance(token, list)])\n",
    "\n",
    "# Generate a response from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "\t\t**inputs,\n",
    "\t\tmax_length=1024,\n",
    "\t\tdo_sample=False,\n",
    "\t)\n",
    "    # Filter out default special tokens from the generated output\n",
    "    filtered_tokens = [token for token in outputs[0] if token not in default_special_token_ids]\n",
    "\n",
    "    # Decode tÂ§he output while skipping all special tokens except \"\\n\"\n",
    "    decoded_output = tokenizer.decode(filtered_tokens, skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "# Decode and print the result\n",
    "print(f\"<User Query>:\\n{input_text}\")\n",
    "print(f\"\\n<Assistant>:\\n{decoded_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
